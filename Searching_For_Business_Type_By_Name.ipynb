{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for a Word in Business Names\n",
    "In this notebook we will look at how to identify certain type of business, just by the name part. For example, say we want to identify businesses that are likely grocery stores. Simply checking if the business name has grocery in it would be too narrow. We want to find other words that appear in business name data that have similar semantic meaning that we could search for. Having these different words, will allow our search to be more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "from collections import OrderedDict\n",
    "\n",
    "import annoy\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "from spacy.lemmatizer import Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will just read in the data, we will just put it in a dictionary.  \n",
    "The data came from the following kaggle dataset: https://www.kaggle.com/peopledatalabssf/free-7-million-company-dataset\n",
    "This is 7MM business from around the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should be using something like pandas, but sometimes it's fun to just use the base python csv package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"companies_sorted.csv\", \"r\") as bnm_csv:\n",
    "    csv_reader = csv.DictReader(bnm_csv)\n",
    "    bus_dat = OrderedDict()\n",
    "    for idx, r in enumerate(csv_reader):\n",
    "        if idx == 0:\n",
    "            for col in r:\n",
    "                bus_dat[col] = [r[col]]\n",
    "        else:\n",
    "            for col in r:\n",
    "                bus_dat[col].append(r[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'name',\n",
       " 'domain',\n",
       " 'year founded',\n",
       " 'industry',\n",
       " 'size range',\n",
       " 'locality',\n",
       " 'country',\n",
       " 'linkedin url',\n",
       " 'current employee estimate',\n",
       " 'total employee estimate']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col_name for col_name in bus_dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibm, ibm.com\n",
      "tata consultancy services, tcs.com\n",
      "accenture, accenture.com\n",
      "us army, goarmy.com\n",
      "ey, ey.com\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(bus_dat[\"name\"][0:5], bus_dat[\"domain\"][0:5]):\n",
    "    print(f\"{i}, {j}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we only need the first columns, which is the business name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_name = bus_dat[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7173426"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bus_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will go through the list, and split all items on spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_parts = [\n",
    "    word for name_list in [name.split() for name in bus_name] for word in name_list\n",
    "]\n",
    "\n",
    "punc_n_nums = string.punctuation + \"\".join([str(i) for i in range(10)])\n",
    "# Strip punctuation and numbers\n",
    "bus_parts = [s.translate(str.maketrans(\"\", \"\", string.punctuation)) for s in bus_parts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['promobroker',\n",
       " 'agente',\n",
       " 'de',\n",
       " 'seguros',\n",
       " 'y',\n",
       " 'de',\n",
       " 'fianzas',\n",
       " 's',\n",
       " 'a',\n",
       " 'de']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_parts[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21455275"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bus_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_parts = list(set([s.lower() for s in bus_parts if s != \"\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to drop all stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Names with SpaCy\n",
    "Next we will process the texts using spacy. A few things we want to do.\n",
    " - Remove stop words.\n",
    " - lemmatize words\n",
    " \n",
    "After this we will dedup the words, and then get the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_words = list(set([s for s in bus_parts if s not in nlp.Defaults.stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will lemmatize all of the word parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = Lemmatizer(nlp.vocab.lookups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will lemmatize the word if it's in the vocab, otherwise just return the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('go', 'care', 'someCrazyWord')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lookup(\"going\"), lemmatizer.lookup(\"caring\"), lemmatizer.lookup(\n",
    "    \"someCrazyWord\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_lemma = [lemmatizer.lookup(w) for w in proc_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop dupicates\n",
    "lemma_sub = list(set(proc_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2158126"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemma_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will go through, and add all of our words, to a dictionary were the lemma is the key, and the word vector is the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd = {}\n",
    "for idx, w in enumerate(lemma_sub):\n",
    "    if nlp.vocab[w].has_vector:\n",
    "        bwd[w] = (idx, nlp.vocab[w].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vectors provide a way for us to represent words in vector space. One of the most common models for creating them is Word2Vec. The closer a word is to another word in vector space, the more similar the meanings are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cosine_similarity(w1, w2, model):\n",
    "    return 1 - cosine(model.vocab[w1].vector, model.vocab[w2].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other text similarity measurements, such as edit distance, or soundex are looking to see if the word has similar spelling. Word vectors consider semantic similarity instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06580065190792084"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cosine_similarity(\"farmer\", \"framer\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5305896997451782"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cosine_similarity(\"farmer\", \"agriculture\", nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Word Vectors with Annoy\n",
    "Now we will query our word vectors using the package Annoy: https://github.com/spotify/annoy  \n",
    "This is a fabulous approximate nearest neighbors package that I use a lot for querying word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "aidx = annoy.AnnoyIndex(300, \"angular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in bwd.values():\n",
    "    aidx.add_item(*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aidx.build(n_trees=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query the index and find the most similar words to our word of interest, \"grocery\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grocery',\n",
       " 'grocer',\n",
       " 'newsagent',\n",
       " 'hypermarket',\n",
       " 'healthfood',\n",
       " 'minimart',\n",
       " 'waitrose',\n",
       " 'newsagency',\n",
       " 'supermarket',\n",
       " 'supercenters',\n",
       " 'store',\n",
       " 'delicatessen',\n",
       " 'döner',\n",
       " 'enoteca',\n",
       " 'presliced',\n",
       " 'bottleshop',\n",
       " 'luncheonette',\n",
       " 'knish',\n",
       " 'bodega',\n",
       " 'carryout']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groc_words = [lemma_sub[i] for i in aidx.get_nns_by_item(bwd[\"grocery\"][0], 20)]\n",
    "groc_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not all of these words would exactly identify a grocery store, but it still provides us with more alternatives than just searching \"grocery\" alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Search Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's grab any variations of these words that may exist in our pre-lemmatized data. We will add these variations to our search terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = {}\n",
    "for w in groc_words:\n",
    "    search_terms[w] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_tuples = [(i, j) for i, j in zip(proc_lemma, proc_words) if i in groc_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('newsagent', 'newsagent'),\n",
       " ('bodega', 'bodegas'),\n",
       " ('hypermarket', 'hypermarket'),\n",
       " ('supermarket', 'supermarkets')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_tuples[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will append all of these variations into our search term dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in lemma_tuples:\n",
    "    search_terms[i].append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grocery': ['groceries', 'grocery'],\n",
       " 'grocer': ['grocers', 'grocer'],\n",
       " 'newsagent': ['newsagent', 'newsagents'],\n",
       " 'hypermarket': ['hypermarket', 'hypermarkets'],\n",
       " 'healthfood': ['healthfood'],\n",
       " 'minimart': ['minimart'],\n",
       " 'waitrose': ['waitrose'],\n",
       " 'newsagency': ['newsagency'],\n",
       " 'supermarket': ['supermarkets', 'supermarket'],\n",
       " 'supercenters': ['supercenters'],\n",
       " 'store': ['stored', 'storing', 'store', 'stores'],\n",
       " 'delicatessen': ['delicatessen', 'delicatessens'],\n",
       " 'döner': ['döner'],\n",
       " 'enoteca': ['enoteca'],\n",
       " 'presliced': ['presliced'],\n",
       " 'bottleshop': ['bottleshop'],\n",
       " 'luncheonette': ['luncheonette'],\n",
       " 'knish': ['knish'],\n",
       " 'bodega': ['bodegas', 'bodega'],\n",
       " 'carryout': ['carryout']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am actually going to drop the \"store\" and \"newsagency\" terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "del search_terms[\"newsagent\"]\n",
    "del search_terms[\"newsagency\"]\n",
    "del search_terms[\"store\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I was doing this for a formal project, I would probably thing through the best way to use these word variations to query the business names. Additionally I would try to also identify common misspellings of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_name_parts = [p for var in search_terms.values() for p in var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make we only are considering the word if it is the actual word, and not a subword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_pattern = r\"|\".join([f\"^{i}$| {i}$| {i} |^{i} \" for i in grocery_name_parts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_stores = [\n",
    "    name for name in bus_dat[\"name\"] if bool(re.search(grocery_pattern, name))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grocery_stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wm morrison supermarkets plc',\n",
       " 'waitrose',\n",
       " 'c&s wholesale grocers',\n",
       " 'hannaford supermarkets',\n",
       " 'woolworths supermarkets',\n",
       " 'price chopper supermarkets',\n",
       " 'ralphs grocery company',\n",
       " 'shoprite supermarkets',\n",
       " \"shaw's supermarkets\",\n",
       " 'save mart supermarkets',\n",
       " 'southeastern grocers',\n",
       " 'brookshire grocery company',\n",
       " 'associated wholesale grocers',\n",
       " 'grocery outlet',\n",
       " 'supermarket']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grocery_stores[0:15]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
