{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Testing out DataTable\n",
    "In this post, I am taking a look at the python `datatable` package. I cannot express how much I __LOVE__ the R `data.table` package. It single handedly made my R workflow. It's fast, memory efficient, has a fun community, and a pleasent API. As I have started using python as my main data science language I have longed for a tool as succinct, and efficient for handling large data in python as the `data.table` package is for R.  \n",
    "Now obviously, when you mention tabular data and python, the first thing you will likely think of is the `pandas` package. Pandas is a fabulous python library; it's fun, has a huge ecosystem, and couples well with nearly every popular python scientific computing tool. The first critisim people have about pandas is _\"It's so slow! And inefficient\"_ or _\"It's so bloated! And has a lot of warts!\"_. I think these statements are a little unfair. I have been using pandas everyday for the past few years for processing millions of records, and performing data analysis. I think with a package as large, and developed, as pandas it's very possible to find parts of the API that are less efficient than others. So while I would consider all parts of pandas usable, it sometimes takes a little trial and error to use it as efficiently as needed for large (800K-4MM record files) data workflow.  \n",
    "\n",
    "Enter pythons `datatable`... I was immediatly excited about this package when I discovered it. It boasted a similar API, and speed to the R `data.table` pacakge. It had many similar functions, was largely built on a bare metal langauge, and borrowed many of the data handling implementations from it's R counterpart. However, the first no-go for me was that it was unusable on windows. Whether I like it or not, my company heavily utalizes the microsoft ecosystem, so I needed a tool that would build on windows. This put my intersts on the package on hold for a while. I checked in everyonce in a while, untill that day came... issue [1114](https://github.com/h2oai/datatable/issues/1114) \"Support datatable on windows\" was closed! So here we are about a year later, and I am finally getting to testing it out.  \n",
    "\n",
    "A few things I am hoping for from this package that exist in it's R sibling package  (these are largely compared to pandas, because as much as I just said I love pandas, if it had all these things... I wouldn't be looking into the datatable package):  \n",
    "\n",
    "  - __Fast csv IO__: CSV files are one of the most common file types in existence. Sadly, while it is an easy file format to grock (there are values... and they are seperated by commas), there are a lot of tools out their that will write malformed csv files, and on the whole other extreme an increadibly small portion that can actually read most of the csv data found in the wild. The R `data.table::fread` function is a __BEAST__! It rips through gigabytes of csv files like a lightsaber through butter _(much more effective than a KNIFE through butter)_. This is sadly an area that pandas is not quite as proficient in. While the pandas `read_csv` function is great, and actually does a really good job with messy deliminated files, it is _so slow_. Additionally, ever other file it complains about encoding issues (hello `encoding=\"latin1\"`), but maybe this is a difference between how encoding is dealt with between R and python? I am not totally sure.\n",
    "  - __Well supported missing values__: This is certainly a artifiact of the R language itself. R supports missing values for character, integer and float data typtes. Pandas, support a less cohesive list. It has the numpy float `nan` value, and then sometimes uses `None` types for strings, and then other times not... All that to say while I think the project is centralizing on consistent missing values, it is not as cut and dry as the missing types in R are. The R `data.table` package which uses all native R types, has just as consistent handling of missing values.\n",
    "\n",
    "Outside of these points, I am really just hoping for a fun, fast pacakge, that is straightforward to use _(come on `datatable` I believe in you)_.  \n",
    "Let's get started."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Getting That Data In\n",
    "First let's take a look at the IO side of things. This is check box number 1, and one I feel quite posotive about. The `datatable` package implements the R package `fread` function, so it seems like they can easily win with this one, by using a lot of the same functionality."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import datatable as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = dt.fread(\"data/application_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(307511, 122)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tb.shape"
   ]
  },
  {
   "source": [
    "That seemed pretty fast... How does it stack up against the pandas csv reader."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.87 s ± 25.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pd.read_csv(\"data/application_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "178 ms ± 3.17 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dt.fread(\"data/application_train.csv\")"
   ]
  },
  {
   "source": [
    "So right away, we see that the `fread` function is nearly 10 times faster than the pandas equivilant. That's a pretty nice bump in speed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/application_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "562761929"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df.memory_usage(deep=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "562761945"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "sys.getsizeof(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "240272739"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "sys.getsizeof(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}